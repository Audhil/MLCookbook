{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Implementing Different Layers to the NN\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# reset and start the graph\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing different types of layers\n",
    "1. Convolution Layer\n",
    "2. Activation Layer\n",
    "3. Max-Pool Layer\n",
    "4. Fully connected Layer\n",
    "\n",
    "We will generate two different data sets for this script, a 1-D data set (row of data) and a 2-D data set (similar to picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------|\n",
    "# -------------------1D-data-------------------------|\n",
    "# ---------------------------------------------------|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning knobs\n",
    "data_size = 25\n",
    "conv_size = 5\n",
    "maxpool_size = 5\n",
    "stride_size = 1\n",
    "num_outputs = 5\n",
    "\n",
    "# ensure reproducibility\n",
    "seed = 25\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------Convolution function-----------\n",
    "def conv_layer_1d(input_1d, my_filter, my_stride):\n",
    "    \"\"\"conv layer function\"\"\"\n",
    "    # TensorFlow's 'conv2d()' function only works with 4D arrays:\n",
    "    # [batch#, width, height, channels], we have 1 batch, and\n",
    "    # 1 channel, but we do have width AND height this time.\n",
    "    # So next we create the 4D array by inserting dimension 1's.\n",
    "\n",
    "    \"\"\"\n",
    "    making everything as below\n",
    "    input_1d ==> Tensor(\"Placeholder:0\", shape=(25,), dtype=float32)\n",
    "    input_2d ==> Tensor(\"ExpandDims:0\", shape=(1, 25), dtype=float32)\n",
    "    input_3d ==> Tensor(\"ExpandDims_1:0\", shape=(1, 1, 25), dtype=float32)\n",
    "    input_4d ==> Tensor(\"ExpandDims_2:0\", shape=(1, 1, 25, 1), dtype=float32)\"\"\"\n",
    "    # second argument points to the indices\n",
    "    input_2d = tf.expand_dims(input_1d, 0)\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # note down the stride difference\n",
    "    conv_output = tf.nn.conv2d(input_4d, filter=my_filter, strides=[1, my_stride, my_stride, 1], padding='VALID')\n",
    "    # get rid of unnecessary dimensions\n",
    "    conv_output_1d = tf.squeeze(conv_output)\n",
    "    return conv_output_1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------Activation function-----------\n",
    "def activation(input_1d):\n",
    "    return tf.nn.relu(input_1d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------Max Pool--------\n",
    "def max_pool(input_1d, width, my_stride):\n",
    "    # Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
    "    # [batch_size=1, width=1, height=num_input, channels=1]\n",
    "    input_2d = tf.expand_dims(input_1d, 0)\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Perform the max pooling with strides = [1,1,1,1]\n",
    "    # If we wanted to increase the stride on our data dimension, say by\n",
    "    # a factor of '2', we put strides = [1, 1, 2, 1]\n",
    "    # We will also need to specify the width of the max-window ('width')\n",
    "    pool_output = tf.nn.max_pool(input_4d, ksize=[1, 1, width, 1], strides=[1, 1, my_stride, 1], padding='VALID')\n",
    "    # get rid of extra dimensions\n",
    "    pool_output_1d = tf.squeeze(pool_output)\n",
    "    return pool_output_1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------Fully connected--------\n",
    "def fully_connected(input_layer, num_outputs):\n",
    "    # First we find the needed shape of the multiplication weight matrix:\n",
    "    # The dimension will be (length of input) by (num_outputs)\n",
    "    weight_shape = tf.squeeze(tf.stack([tf.shape(input_layer), [num_outputs]]))\n",
    "    # weight\n",
    "    weight = tf.random_normal(weight_shape, stddev=.1)\n",
    "    # bias\n",
    "    bias = tf.random_normal(shape=[num_outputs])\n",
    "    # Make the 1D input array into a 2D array for matrix multiplication\n",
    "    input_layer_2d = tf.expand_dims(input_layer, 0)\n",
    "    # perform matrix multiplication and add bias\n",
    "    full_output = tf.add(bias, tf.matmul(input_layer_2d, weight))\n",
    "    # get rid of unwanted dimenstions\n",
    "    full_output_1d = tf.squeeze(full_output)\n",
    "    return full_output_1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 1D Data <<<<\nInput = array of length 25\nConvolution w/ filter, length = 5, stride size = 1, results in an array of length 21:\nConvolution Layer output : \n [ -3.13537598e-01   3.35255265e-03  -2.51171255e+00   2.69317269e+00\n  -7.15947104e+00   4.47138786e+00  -2.31978464e+00  -9.98717844e-02\n   3.64500523e+00  -5.84662914e+00   2.38776851e+00  -9.71207857e-01\n   3.41508985e+00  -4.75846004e+00   2.65730858e+00  -2.02076888e+00\n   4.84974289e+00  -7.25299644e+00   9.88106918e+00  -1.50379050e+00\n  -3.74822474e+00]\n\nInput = above array of length 21\nReLU element wise returns an array of length 21:\nActivation Layer output : \n [  0.00000000e+00   3.35255265e-03   0.00000000e+00   2.69317269e+00\n   0.00000000e+00   4.47138786e+00   0.00000000e+00   0.00000000e+00\n   3.64500523e+00   0.00000000e+00   2.38776851e+00   0.00000000e+00\n   3.41508985e+00   0.00000000e+00   2.65730858e+00   0.00000000e+00\n   4.84974289e+00   0.00000000e+00   9.88106918e+00   0.00000000e+00\n   0.00000000e+00]\n\nInput = above array of length 21\nMaxPool, window length = 5, stride size = 1, results in the array of length 17\nMaxPool Layer output : \n [ 2.69317269  4.47138786  4.47138786  4.47138786  4.47138786  4.47138786\n  3.64500523  3.64500523  3.64500523  3.41508985  3.41508985  3.41508985\n  4.84974289  4.84974289  9.88106918  9.88106918  9.88106918]\n\nInput = above array of length 17\nFully connected layer on all 4 rows with 5 outputs:\nFullyConnected Layer output : \n [-1.33095956  0.01353824 -6.11985254  0.62515694 -1.88377213]\n"
     ]
    }
   ],
   "source": [
    "data_1d = np.random.normal(size=data_size)\n",
    "\n",
    "# placeholder\n",
    "data_p_holder = tf.placeholder(tf.float32, shape=[data_size])\n",
    "\n",
    "# filter for convolution\n",
    "my_filter = tf.Variable(tf.random_normal(shape=[1, conv_size, 1, 1]))\n",
    "\n",
    "# creating convolution layer\n",
    "my_conv_output = conv_layer_1d(data_p_holder, my_filter, my_stride=stride_size)\n",
    "activation_output = activation(my_conv_output)\n",
    "max_pool_output = max_pool(activation_output, width=maxpool_size, my_stride=stride_size)\n",
    "full_output = fully_connected(max_pool_output, num_outputs)\n",
    "\n",
    "init_vars = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_vars)\n",
    "    feed_dict = {data_p_holder: data_1d}\n",
    "    print('>>>> 1D Data <<<<')\n",
    "    \"\"\"Output of conv_layer_1d\"\"\"\n",
    "    print('Input = array of length %d' % (data_p_holder.shape.as_list()[0]))\n",
    "    print('Convolution w/ filter, length = %d, stride size = %d, results in an array of length %d:' %\n",
    "          (conv_size, stride_size, my_conv_output.shape.as_list()[0]))\n",
    "    print('Convolution Layer output : \\n', sess.run(my_conv_output, feed_dict=feed_dict))\n",
    "\n",
    "    \"\"\"Output of activation_output\"\"\"\n",
    "    print('\\nInput = above array of length %d' % (my_conv_output.shape.as_list()[0]))\n",
    "    print('ReLU element wise returns an array of length %d:' % (activation_output.shape.as_list()[0]))\n",
    "    print('Activation Layer output : \\n', sess.run(activation_output, feed_dict=feed_dict))\n",
    "\n",
    "    \"\"\"Output of max_pool_output\"\"\"\n",
    "    print('\\nInput = above array of length %d' % (activation_output.shape.as_list()[0]))\n",
    "    print('MaxPool, window length = %d, stride size = %d, results in the array of length %d' % (\n",
    "    maxpool_size, stride_size, max_pool_output.shape.as_list()[0]))\n",
    "    print('MaxPool Layer output : \\n', sess.run(max_pool_output, feed_dict=feed_dict))\n",
    "\n",
    "    \"\"\"Output of full_output\"\"\"\n",
    "    print('\\nInput = above array of length %d' % (max_pool_output.shape.as_list()[0]))\n",
    "    print('Fully connected layer on all 4 rows with %d outputs:' % (full_output.shape.as_list()[0]))\n",
    "    print('FullyConnected Layer output : \\n', sess.run(full_output, feed_dict=feed_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------|\n",
    "# -------------------2D-data-------------------------|\n",
    "# ---------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 2D Data <<<<\nInput = [10, 10] array\n[2, 2] Convolution, stride size = [2, 2] , results in the [5, 5] array\n[[-0.53970814 -1.37039399  0.16790187  0.80627304  2.52373457]\n [-3.81318855  3.66644287 -0.88086319 -2.76224279  5.07420492]\n [-2.33438158  0.5242815   0.71433663 -1.56049573 -1.59068143]\n [ 1.18478489 -1.71592748  1.03441024  1.0885272  -0.54076439]\n [ 5.41466427 -4.74507046 -0.94251752 -4.45919561  2.67982888]]\n\nInput = the above [5, 5] array\nReLU element wise returns the [5, 5] array\n[[ 0.          0.          0.16790187  0.80627304  2.52373457]\n [ 0.          3.66644287  0.          0.          5.07420492]\n [ 0.          0.5242815   0.71433663  0.          0.        ]\n [ 1.18478489  0.          1.03441024  1.0885272   0.        ]\n [ 5.41466427  0.          0.          0.          2.67982888]]\n\nInput = the above [5, 5] array\nMaxPool, stride size = [1, 1], results in [4, 4] array\n[[ 3.66644287  3.66644287  0.80627304  5.07420492]\n [ 3.66644287  3.66644287  0.71433663  5.07420492]\n [ 1.18478489  1.03441024  1.0885272   1.0885272 ]\n [ 5.41466427  1.03441024  1.0885272   2.67982888]]\n\nInput = the above [4, 4] array\nFully connected layer on all 4 rows results in 5 outputs:\n[ 0.02771282 -0.59893429  0.42170486  0.99334151  3.11650801]\n"
     ]
    }
   ],
   "source": [
    "row_size = 10\n",
    "col_size = 10\n",
    "conv_size = 2\n",
    "conv_stride_size = 2\n",
    "max_pool_size = 2\n",
    "max_pool_stride_size = 1\n",
    "\n",
    "\n",
    "# --------Convolution--------\n",
    "def conv_2d_layer(input_2d, my_filter, stride_size):\n",
    "    # TensorFlow's 'conv2d()' function only works with 4D arrays:\n",
    "    # [batch#, width, height, channels], we have 1 batch, and\n",
    "    # 1 channel, but we do have width AND height this time.\n",
    "    # So next we create the 4D array by inserting dimension 1's.\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Note the stride difference below!\n",
    "    convolution_output = tf.nn.conv2d(input_4d, my_filter, strides=[1, stride_size, stride_size, 1], padding='VALID')\n",
    "    # get rid or unnecessary dimensions\n",
    "    convolution_2d_output = tf.squeeze(convolution_output)\n",
    "    return convolution_2d_output\n",
    "\n",
    "\n",
    "# --------Activation--------\n",
    "def activation_layer(input_2d):\n",
    "    return tf.nn.relu(input_2d)\n",
    "\n",
    "\n",
    "# --------Max Pool--------\n",
    "def max_pool(input_2d, width, height, stride):\n",
    "    # Just like 'conv2d()' above, max_pool() works with 4D arrays.\n",
    "    # [batch_size=1, width=given, height=given, channels=1]\n",
    "    input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    # Perform the max pooling with strides = [1,1,1,1]\n",
    "    # If we wanted to increase the stride on our data dimension, say by\n",
    "    # a factor of '2', we put strides = [1, 2, 2, 1]\n",
    "    pool_output = tf.nn.max_pool(input_4d, ksize=[1, height, width, 1],\n",
    "                                 strides=[1, stride, stride, 1],\n",
    "                                 padding='VALID')\n",
    "    # Get rid of unnecessary dimensions\n",
    "    pool_output_2d = tf.squeeze(pool_output)\n",
    "    return pool_output_2d\n",
    "\n",
    "\n",
    "# --------Fully Connected--------\n",
    "def fully_connected(input_layer, num_outputs):\n",
    "    # In order to connect our whole W byH 2d array, we first flatten it out to\n",
    "    # a W times H 1D array.\n",
    "    flat_input = tf.reshape(input_layer, [-1])  # making 1D array\n",
    "    # We then find out how long it is, and create an array for the shape of\n",
    "    # the multiplication weight = (WxH) by (num_outputs)\n",
    "    weight_shape = tf.squeeze(tf.stack([tf.shape(flat_input), [num_outputs]]))\n",
    "    # Initialize the weight\n",
    "    weight = tf.random_normal(weight_shape, stddev=0.1)\n",
    "    # Initialize the bias\n",
    "    bias = tf.random_normal(shape=[num_outputs])\n",
    "    # Now make the flat 1D array into a 2D array for multiplication\n",
    "    input_2d = tf.expand_dims(flat_input, 0)\n",
    "    # Multiply and add the bias\n",
    "    full_output = tf.add(tf.matmul(input_2d, weight), bias)\n",
    "    # Get rid of extra dimension\n",
    "    full_output_2d = tf.squeeze(full_output)\n",
    "    return full_output_2d\n",
    "\n",
    "\n",
    "# ensure reproducibility\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Generate 2D data\n",
    "data_size = [row_size, col_size]\n",
    "data_2d = np.random.normal(size=data_size)\n",
    "\n",
    "# placeholder\n",
    "data_2d_place_holder = tf.placeholder(dtype=tf.float32, shape=data_size)\n",
    "\n",
    "# convolution filter\n",
    "my_filter = tf.Variable(tf.random_normal(shape=[conv_size, conv_size, 1, 1]))\n",
    "\n",
    "# convolution layer\n",
    "my_conv_output = conv_2d_layer(data_2d_place_holder, my_filter, stride_size=conv_stride_size)\n",
    "\n",
    "# create activation layer\n",
    "my_activation_output = activation_layer(my_conv_output)\n",
    "\n",
    "# create maxpool layer\n",
    "my_maxpool_output = max_pool(my_activation_output, max_pool_size, max_pool_size, stride=max_pool_stride_size)\n",
    "\n",
    "# Create Fully Connected Layer\n",
    "my_full_output = fully_connected(my_maxpool_output, num_outputs)\n",
    "\n",
    "# init vars\n",
    "init_vars = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_vars)\n",
    "    feed_dict = {data_2d_place_holder: data_2d}\n",
    "\n",
    "    print('>>>> 2D Data <<<<')\n",
    "\n",
    "    # Convolution Output\n",
    "    print('Input = %s array' % (data_2d_place_holder.shape.as_list()))\n",
    "    print('%s Convolution, stride size = [%d, %d] , results in the %s array' %\n",
    "          (my_filter.get_shape().as_list()[:2], conv_stride_size, conv_stride_size,\n",
    "           my_conv_output.shape.as_list()))\n",
    "    print(sess.run(my_conv_output, feed_dict=feed_dict))\n",
    "\n",
    "    # Activation Output\n",
    "    print('\\nInput = the above %s array' % (my_conv_output.shape.as_list()))\n",
    "    print('ReLU element wise returns the %s array' % (my_activation_output.shape.as_list()))\n",
    "    print(sess.run(my_activation_output, feed_dict=feed_dict))\n",
    "\n",
    "    # Max Pool Output\n",
    "    print('\\nInput = the above %s array' % (my_activation_output.shape.as_list()))\n",
    "    print('MaxPool, stride size = [%d, %d], results in %s array' %\n",
    "          (max_pool_stride_size, max_pool_stride_size, my_maxpool_output.shape.as_list()))\n",
    "    print(sess.run(my_maxpool_output, feed_dict=feed_dict))\n",
    "\n",
    "    # Fully Connected Output\n",
    "    print('\\nInput = the above %s array' % (my_maxpool_output.shape.as_list()))\n",
    "    print('Fully connected layer on all %d rows results in %s outputs:' %\n",
    "          (my_maxpool_output.shape.as_list()[0], my_full_output.shape.as_list()[0]))\n",
    "    print(sess.run(my_full_output, feed_dict=feed_dict))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
