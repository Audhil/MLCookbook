{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blog @ https://medium.com/towards-data-science/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac\n",
    "# only 3 layers NN\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', 'queen', 'is', 'the', 'king', 'royal', 'he'}\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
    "corpus_raw = corpus_raw.lower()\n",
    "\n",
    "# dictionary translates words to integers, vice versa\n",
    "words = []\n",
    "\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.':  # avoiding '.' as word\n",
    "        words.append(word)\n",
    "words = set(words)  # avoiding duplicates\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(words)\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "\n",
    "# word2vec - refer : http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "# window size = 2\n",
    "# \n",
    "# source text\t\t\t\t\t\t\t\t\t\t    training samples\n",
    "# The quick brown fox jumps over the lazy dog\t\t\t    (the, quick)\n",
    "# ---------------       \t\t\t\t\t\t\t\t\t(the, brown)\n",
    "# \n",
    "# The quick brown fox jumps over the lazy dog\t\t\t    (quick, the)\n",
    "# -------------------        \t\t\t\t\t\t\t\t(quick, brown)\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t            (quick, fox)\n",
    "# \n",
    "# The quick brown fox jumps over the lazy dog\t\t\t    (brown, the)\n",
    "# -------------------------         \t\t\t\t\t\t(brown, quick)\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t            (brown, fox)\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t            (brown, jumps)\n",
    "# \n",
    "# \n",
    "# The quick brown fox jumps over the lazy dog\t\t\t    (fox, quick)\n",
    "# \t  --------------------------        \t\t\t\t\t(fox, brown)\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t            (fox, jumps)\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t            (fox, over)\n",
    "# Note: If the word is at the beginning or ending of sentence, the window ignores the outer words.\n",
    "\n",
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "# this is brilliant\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0): min(word_index + WINDOW_SIZE, len(sentence)) + 1]:\n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "\n",
    "# training data ==> data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot_vector(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n",
    "\n",
    "x_train = []  # input word\n",
    "y_train = []  # output word\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot_vector(word2int[data_word[0]], vocab_size))\n",
    "    y_train.append(to_one_hot_vector(word2int[data_word[1]], vocab_size))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .1\n",
    "epochs = 10000\n",
    "\n",
    "# making placeholders for x_train and y_train\n",
    "X = tf.placeholder(tf.float32, shape=[None, vocab_size])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, vocab_size])\n",
    "\n",
    "# we take our training data and convert into the embedded representation.\n",
    "EMBEDDING_DIM = 5  # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))  #bias\n",
    "hidden_representation = tf.add(tf.matmul(X, W1), b1)\n",
    "\n",
    "# Next, we take what we have in the embedded dimension and make a prediction about the neighbour. \n",
    "# To make the prediction we use softmax.\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation, W2), b2))\n",
    "\n",
    "# define the loss function:\n",
    "# Hy′(y):=−∑iy′ilog(yi)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# train step\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "init_var = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1771524  -1.58012688  1.69621992  1.2537359  -1.51709461]\n"
     ]
    }
   ],
   "source": [
    "# train block\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_var)\n",
    "    for _ in range(epochs):\n",
    "        sess.run(train_step, feed_dict={X: x_train, Y: y_train})\n",
    "        \"\"\"It eventually stabilises on a constant loss. Even though we can’t get high accuracy, \n",
    "        we don’t care. All we are interested in is W1 and b1, i.e., the hidden representations.\"\"\"\n",
    "        # print('loss is : ', sess.run(cross_entropy_loss, feed_dict={X: x_train, Y: y_train}))\n",
    "    # print('W1 :: ', sess.run(W1))\n",
    "    # print('b1 :: ', sess.run(b1))\n",
    "    \"\"\"When we multiply the one hot vectors with W1 , \n",
    "    we basically get access to the row of the of W1 \n",
    "    which is in fact the embedded representation of the word represented by the input one hot vector. \n",
    "    So W1 is essentially acting as a look up table.\"\"\"\n",
    "\n",
    "    vectors = sess.run(W1 + b1)\n",
    "    # if you work it out, you will see that it has the same effect as running the node hidden representation\n",
    "    # print(vectors)\n",
    "    # If we want the representation for ‘queen’, all we have to do is:\n",
    "    print(vectors[word2int['queen']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
