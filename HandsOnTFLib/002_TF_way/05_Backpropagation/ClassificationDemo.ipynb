{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification example, we will create an x-sample made of two different normal distribution inputs, Normal(mean = -1, sd = 1) and Normal(mean = 3, sd = 1). For each of these the target will be the class 0 or 1 respectively.\n",
    "\n",
    "The model will fit the binary classification: \n",
    "If sigmoid(x+A) < 0.5 then predict class 0, else class 1.\n",
    "\n",
    "Theoretically, we know that A should take on the value of the negative average of the two \n",
    "means: -(mean1 + mean2)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .02\n",
    "display_step = 25\n",
    "epochs = 1400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25  A is [ 9.43274212] Loss = [[ 9.65841675]]\nstep 50  A is [ 8.93292141] Loss = [[ 7.34313822]]\nstep 75  A is [ 8.43325138] Loss = [[ 6.46834183]]\nstep 100  A is [ 7.93367958] Loss = [[ 6.33662701]]\nstep 125  A is [ 7.43453074] Loss = [[ 6.0590086]]\nstep 150  A is [ 6.93569946] Loss = [[ 6.19348764]]\nstep 175  A is [ 6.4384079] Loss = [[ 4.28733158]]\nstep 200  A is [ 5.94210482] Loss = [[ 4.2555604]]\nstep 225  A is [ 5.44800282] Loss = [[ 3.69847274]]\nstep 250  A is [ 4.95889759] Loss = [[ 3.60850215]]\nstep 275  A is [ 4.47422123] Loss = [[ 2.66961527]]\nstep 300  A is [ 4.00130606] Loss = [[ 2.23773718]]\nstep 325  A is [ 3.54678988] Loss = [[ 1.76755989]]\nstep 350  A is [ 3.10275674] Loss = [[ 1.29642904]]\nstep 375  A is [ 2.68494797] Loss = [[ 1.70173883]]\nstep 400  A is [ 2.2892561] Loss = [[ 1.59259081]]\nstep 425  A is [ 1.91257977] Loss = [[ 2.78351235]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 450  A is [ 1.57837534] Loss = [[ 1.95639896]]\nstep 475  A is [ 1.32512879] Loss = [[ 0.72179651]]\nstep 500  A is [ 1.03672278] Loss = [[ 0.39077529]]\nstep 525  A is [ 0.81836498] Loss = [[ 0.21839884]]\nstep 550  A is [ 0.62749362] Loss = [[ 1.20804071]]\nstep 575  A is [ 0.44041103] Loss = [[ 0.31976193]]\nstep 600  A is [ 0.27652711] Loss = [[ 0.30310482]]\nstep 625  A is [ 0.09808667] Loss = [[ 0.51646864]]\nstep 650  A is [-0.05646053] Loss = [[ 1.1497426]]\nstep 675  A is [-0.19251294] Loss = [[ 0.37621051]]\nstep 700  A is [-0.324431] Loss = [[ 0.73772955]]\nstep 725  A is [-0.44052878] Loss = [[ 0.33256534]]\nstep 750  A is [-0.54641676] Loss = [[ 0.15283218]]\nstep 775  A is [-0.66286218] Loss = [[ 0.10062933]]\nstep 800  A is [-0.74543804] Loss = [[ 0.11974307]]\nstep 825  A is [-0.82190436] Loss = [[ 0.43880281]]\nstep 850  A is [-0.90486026] Loss = [[ 0.06011653]]\nstep 875  A is [-0.96982729] Loss = [[ 0.10131311]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900  A is [-1.02040732] Loss = [[ 0.02594052]]\nstep 925  A is [-1.09402812] Loss = [[ 0.17009802]]\nstep 950  A is [-1.15616608] Loss = [[ 0.15952557]]\nstep 975  A is [-1.22339046] Loss = [[ 0.09779515]]\nstep 1000  A is [-1.28172636] Loss = [[ 0.0562189]]\nstep 1025  A is [-1.33383715] Loss = [[ 0.15269013]]\nstep 1050  A is [-1.38863492] Loss = [[ 0.01802127]]\nstep 1075  A is [-1.42441249] Loss = [[ 0.07755689]]\nstep 1100  A is [-1.47110784] Loss = [[ 0.07414652]]\nstep 1125  A is [-1.51752782] Loss = [[ 0.40579003]]\nstep 1150  A is [-1.55292451] Loss = [[ 0.02253191]]\nstep 1175  A is [-1.59509683] Loss = [[ 0.19778366]]\nstep 1200  A is [-1.64150751] Loss = [[ 0.36056504]]\nstep 1225  A is [-1.68904448] Loss = [[ 0.03775949]]\nstep 1250  A is [-1.71988094] Loss = [[ 0.10986589]]\nstep 1275  A is [-1.764341] Loss = [[ 0.04071258]]\nstep 1300  A is [-1.81014359] Loss = [[ 0.18651569]]\nstep 1325 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A is [-1.85487199] Loss = [[ 0.01855124]]\nstep 1350  A is [-1.89137876] Loss = [[ 0.21582186]]\nstep 1375  A is [-1.9220134] Loss = [[ 0.28817096]]\nstep 1400  A is [-1.95079923] Loss = [[ 0.02918898]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending accuracy :  0.91\n"
     ]
    }
   ],
   "source": [
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(3, 1, 50)))\n",
    "y_vals_classes = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
    "\n",
    "# variable\n",
    "A = tf.Variable(tf.random_normal(mean=10, shape=[1]), name='A')\n",
    "\n",
    "# placeholder\n",
    "x_place_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "y_place_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "\n",
    "addition = tf.add(x_place_holder, A)\n",
    "\n",
    "# We also have to add the batch dimension to each of the target and input values \n",
    "# to use the built in functions.\n",
    "\n",
    "addition_expanded = tf.expand_dims(addition, 0)\n",
    "y_place_holder_expanded = tf.expand_dims(y_place_holder, 0)\n",
    "\n",
    "# loss/cost function\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=addition_expanded, labels=y_place_holder_expanded)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init_vars = tf.global_variables_initializer()\n",
    "\n",
    "# training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_vars)\n",
    "    for i in range(epochs):\n",
    "        random_choice = np.random.choice(50)\n",
    "        sess.run(optimizer, feed_dict={x_place_holder: [x_vals[random_choice]],\n",
    "                                       y_place_holder: [y_vals_classes[random_choice]]})\n",
    "\n",
    "        if (i + 1) % display_step == 0:\n",
    "            print('step %d ' % (i + 1),\n",
    "                  'A is ' + str(sess.run(A)),\n",
    "                  'Loss = ' + str(sess.run(loss, feed_dict={x_place_holder: [x_vals[random_choice]],\n",
    "                                                            y_place_holder: [y_vals_classes[random_choice]]})))\n",
    "\n",
    "    # Now we can also see how well we did at \n",
    "    # predicting the data by creating an accuracy function and evaluating them on the known targets.\n",
    "    predictions = []\n",
    "    for i in range(len(x_vals)):\n",
    "        x_val = [x_vals[i]]\n",
    "        prediction = sess.run(tf.round(tf.sigmoid(addition)), feed_dict={x_place_holder: x_val})\n",
    "        predictions.append(prediction[0])\n",
    "\n",
    "    sum_of_trues = sum(x == y for x, y in zip(predictions, y_vals_classes))  # sums no. of 'True's when (x == y)\n",
    "    accuracy = sum_of_trues / 100.\n",
    "    print('Ending accuracy : ', str(np.round(accuracy, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
