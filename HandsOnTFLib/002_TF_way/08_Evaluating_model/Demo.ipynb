{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluating Models\"\"\"\n",
    "\n",
    "# source code : https://github.com/Audhil/tensorflow_cookbook/blob/master/02_TensorFlow_Way/08_Evaluating_Models/08_evaluating_models.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the regression model we will generate 100 random samples from a Normal(mean=1, sd=0.1). \\nThe target will be an array of size 100 filled with the target value of 10.0.\\nWe will fit the linear model $y=A \\\\cdot x$ (no y intercept). The theoretical value of A is 10.0.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Regression Model\"\"\"\n",
    "\n",
    "\"\"\"For the regression model we will generate 100 random samples from a Normal(mean=1, sd=0.1). \n",
    "The target will be an array of size 100 filled with the target value of 10.0.\n",
    "We will fit the linear model $y=A \\cdot x$ (no y intercept). The theoretical value of A is 10.0.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "learning_rate = .02\n",
    "epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #25 A = [[ 6.79948568]]\nLoss = 10.3066\nStep #50 A = [[ 8.84168816]]\nLoss = 1.34861\nStep #75 A = [[ 9.57591248]]\nLoss = 1.20772\nStep #100 A = [[ 9.82526588]]\nLoss = 0.514096\nStep #125 A = [[ 9.91490555]]\nLoss = 0.572136\nStep #150 A = [[ 9.95020294]]\nLoss = 1.32269\nStep #175 A = [[ 9.97117138]]\nLoss = 1.13945\nStep #200 A = [[ 9.97158527]]\nLoss = 0.47447\nStep #225 A = [[ 10.00874519]]\nLoss = 1.17678\nStep #250 A = [[ 10.02682114]]\nLoss = 0.76478\nStep #275 A = [[ 9.97177792]]\nLoss = 0.84327\nStep #300 A = [[ 9.95977497]]\nLoss = 1.01776\nStep #325 A = [[ 9.92786503]]\nLoss = 0.453751\nStep #350 A = [[ 9.96060181]]\nLoss = 0.677054\nStep #375 A = [[ 9.95635319]]\nLoss = 0.753239\nStep #400 A = [[ 9.98184586]]\nLoss = 0.624961\nStep #425 A = [[ 9.99627399]]\nLoss = 0.999815\nStep #450 A = [[ 10.00660801]]\nLoss = 0.686894\nStep #475 A = [[ 9.97324085]]\nLoss = 0.634348\nStep #500 A = [[ 10.0066576]]\nLoss = 0.762524\nStep #525 A = [[ 10.03481865]]\nLoss = 0.686287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #550 A = [[ 10.01509476]]\nLoss = 0.826554\nStep #575 A = [[ 10.03890419]]\nLoss = 1.0503\nStep #600 A = [[ 10.07072735]]\nLoss = 0.652695\nStep #625 A = [[ 10.03097916]]\nLoss = 0.617692\nStep #650 A = [[ 10.00998592]]\nLoss = 0.589343\nStep #675 A = [[ 10.04493809]]\nLoss = 0.717396\nStep #700 A = [[ 10.04192257]]\nLoss = 1.21304\nStep #725 A = [[ 9.99537659]]\nLoss = 0.924513\nStep #750 A = [[ 9.97947121]]\nLoss = 0.790202\nStep #775 A = [[ 10.00146675]]\nLoss = 0.598803\nStep #800 A = [[ 10.01720428]]\nLoss = 0.462609\nStep #825 A = [[ 10.05621052]]\nLoss = 0.769973\nStep #850 A = [[ 10.01642704]]\nLoss = 0.72248\nStep #875 A = [[ 10.0206871]]\nLoss = 1.09169\nStep #900 A = [[ 10.02209473]]\nLoss = 0.710216\nStep #925 A = [[ 10.01005363]]\nLoss = 1.03608\nStep #950 A = [[ 9.97808075]]\nLoss = 0.79885\nStep #975 A = [[ 9.988451]]\nLoss = 0.818484\nStep #1000 A = [[ 10.0209198]]\nLoss = 0.757188\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate Data for Regression\n",
    "Here we generate the data required for the regression. We also specify the necessary placeholders.\n",
    "After we split the data into a 80-20 train-test split.\"\"\"\n",
    "\n",
    "# Create data\n",
    "x_vals = np.random.normal(1, 0.1, 100)\n",
    "y_vals = np.repeat(10., 100)\n",
    "\n",
    "# Split data into train/test = 80%/20%\n",
    "train_indices = np.random.choice(len(x_vals), round(len(x_vals) * .8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]\n",
    "\n",
    "# place holders\n",
    "x_place_holder = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "y_place_holder = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "# Variable\n",
    "A = tf.Variable(tf.random_normal(shape=[1, 1]))\n",
    "\n",
    "# model\n",
    "model = tf.matmul(x_place_holder, A)\n",
    "\n",
    "# loss function\n",
    "# L2 loss\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(model, y_place_holder)))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init_vars = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_vars)\n",
    "    for i in range(epochs):\n",
    "        rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "        rand_x = np.transpose([x_vals_train[rand_index]])\n",
    "        rand_y = np.transpose([y_vals_train[rand_index]])\n",
    "        sess.run(optimizer, feed_dict={x_place_holder: rand_x, y_place_holder: rand_y})\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)))\n",
    "            print('Loss = ' + str(sess.run(loss, feed_dict={x_place_holder: rand_x, y_place_holder: rand_y})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on train :  0.885299987793\nMSE on test :  0.884499969482\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluating the Regression model\"\"\"\n",
    "# For the regression model evaluation, we will run the loss wih the training and test set.\n",
    "\n",
    "# Evaluate accuracy (loss) on test set\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_vars)\n",
    "    mse_train = sess.run(loss, feed_dict={x_place_holder: np.transpose([x_vals_train]),\n",
    "                                          y_place_holder: np.transpose([y_vals_train])})\n",
    "\n",
    "    mse_test = sess.run(loss, feed_dict={x_place_holder: np.transpose([x_vals_test]),\n",
    "                                         y_place_holder: np.transpose([y_vals_test])})\n",
    "\n",
    "    print('MSE on train : ', str(np.round(mse_train, 2) / 100))\n",
    "    print('MSE on test : ', str(np.round(mse_test, 2) / 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Example\n",
    "\n",
    "For the classification example, we generate data as follows:\n",
    "The input data will be a sample of size 50 from a Normal(mean = -1, sd = 1) and a sample of 50 from a Normal(mean = 1, sd = 1).\n",
    "The target data will be 50 values of 0 and 50 values of 1.\n",
    "We fit the binary classification model:\n",
    "If $sigmoid(x+A)&lt;0.5$ Then we predict class 0\n",
    "If $sigmoid(x+A)&gt;=0.5$ Then we predict class 1\n",
    "Theoretically A should be\n",
    "$$ - \\frac{mean1 + mean2}{2} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #200 A = [ 4.75223684]\nLoss = 2.75295\nStep #400 A = [ 1.03689408]\nLoss = 0.553718\nStep #600 A = [-0.02879105]\nLoss = 0.220092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #800 A = [-0.23992744]\nLoss = 0.242676\nStep #1000 A = [-0.3415409]\nLoss = 0.26192\nStep #1200 A = [-0.29597193]\nLoss = 0.351862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1400 A = [-0.3221682]\nLoss = 0.19834\nStep #1600 A = [-0.31789702]\nLoss = 0.247198\nStep #1800 A = [-0.30745864]\nLoss = 0.319557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.9125\nAccuracy on test set: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAB7rntwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bb73e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "batch_size = 25\n",
    "\n",
    "# Create data\n",
    "x_vals = np.concatenate((np.random.normal(-1, 1, 50), np.random.normal(2, 1, 50)))\n",
    "y_vals = np.concatenate((np.repeat(0., 50), np.repeat(1., 50)))\n",
    "x_data = tf.placeholder(shape=[1, None], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[1, None], dtype=tf.float32)\n",
    "\n",
    "# Split data into train/test = 80%/20%\n",
    "train_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\n",
    "x_vals_train = x_vals[train_indices]\n",
    "x_vals_test = x_vals[test_indices]\n",
    "y_vals_train = y_vals[train_indices]\n",
    "y_vals_test = y_vals[test_indices]\n",
    "\n",
    "# Create variable (one model parameter = A)\n",
    "A = tf.Variable(tf.random_normal(mean=10, shape=[1]))\n",
    "\n",
    "# Add operation to graph\n",
    "# Want to create the operstion sigmoid(x + A)\n",
    "# Note, the sigmoid() part is in the loss function\n",
    "my_output = tf.add(x_data, A)\n",
    "\n",
    "# Add classification loss (cross entropy)\n",
    "xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=my_output, labels=y_target))\n",
    "\n",
    "# Create Optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = my_opt.minimize(xentropy)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Run loop\n",
    "for i in range(1800):\n",
    "    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n",
    "    rand_x = [x_vals_train[rand_index]]\n",
    "    rand_y = [y_vals_train[rand_index]]\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    if (i + 1) % 200 == 0:\n",
    "        print('Step #' + str(i + 1) + ' A = ' + str(sess.run(A)))\n",
    "        print('Loss = ' + str(sess.run(xentropy, feed_dict={x_data: rand_x, y_target: rand_y})))\n",
    "\n",
    "# evaluation of Classification\n",
    "# Evaluate Predictions on test set\n",
    "y_prediction = tf.squeeze(tf.round(tf.nn.sigmoid(tf.add(x_data, A))))\n",
    "correct_prediction = tf.equal(y_prediction, y_target)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "acc_value_test = sess.run(accuracy, feed_dict={x_data: [x_vals_test], y_target: [y_vals_test]})\n",
    "acc_value_train = sess.run(accuracy, feed_dict={x_data: [x_vals_train], y_target: [y_vals_train]})\n",
    "print('Accuracy on train set: ' + str(acc_value_train))\n",
    "print('Accuracy on test set: ' + str(acc_value_test))\n",
    "\n",
    "# Plot classification result\n",
    "A_result = -sess.run(A)\n",
    "bins = np.linspace(-5, 5, 50)\n",
    "plt.hist(x_vals[0:50], bins, alpha=0.5, label='N(-1,1)', color='white')\n",
    "plt.hist(x_vals[50:100], bins[0:50], alpha=0.5, label='N(2,1)', color='red')\n",
    "plt.plot((A_result, A_result), (0, 8), 'k--', linewidth=3, label='A = ' + str(np.round(A_result, 2)))\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Binary Classifier, Accuracy=' + str(np.round(acc_value_test, 2)))\n",
    "plt.show()\n",
    "\n",
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
